# Домашнее задание к занятию "`Репликация и масштабирование. Часть 2`" - `Юрочкин В.А.`

---

### Задание 1

Опишите основные преимущества использования масштабирования методами:

активный master-сервер и пассивный репликационный slave-сервер;
master-сервер и несколько slave-серверов;

`Ответ:`

## Активный master-сервер и пассивный репликационный slave-сервер

В данной архитектуре один сервер выполняет роль активного master-узла, на котором обрабатываются все операции записи, а второй сервер работает как пассивный slave, получающий данные через механизм репликации.

Основные преимущества данного подхода:

- **Повышение отказоустойчивости**  
  Slave содержит актуальную копию данных master-сервера и может быть использован для восстановления сервиса при выходе master из строя.

- **Резервное копирование без нагрузки на master**  
  Бэкапы и служебные операции (дампы, проверки целостности данных) можно выполнять на slave-сервере, не влияя на производительность основного узла.

- **Разгрузка master-сервера**  
  Часть операций чтения и аналитических запросов может быть перенесена на slave, что снижает нагрузку на master.

- **Простота архитектуры**  
  Схема проста в настройке и сопровождении, не требует сложной логики балансировки или разрешения конфликтов.

- **Безопасность и контроль изменений**  
  Все операции записи сосредоточены на одном сервере, что исключает конфликты данных и упрощает контроль целостности базы.

---

## Master-сервер и несколько slave-серверов

В этой схеме один master-сервер обслуживает операции записи, а несколько slave-серверов используются для репликации и обработки запросов на чтение.

Основные преимущества данного подхода:

- **Горизонтальное масштабирование чтения**  
  Запросы SELECT распределяются между несколькими slave-серверами, что позволяет обслуживать большое количество клиентов и повышает общую производительность системы.

- **Высокая отказоустойчивость**  
  При выходе из строя одного slave-сервера система продолжает работать за счёт остальных узлов. Также возможно быстрое повышение одного из slave до роли master.

- **Гибкое распределение нагрузки**  
  Разные slave-серверы могут использоваться для различных задач: отчёты, аналитика, API, бэкапы.

- **Повышение стабильности master-сервера**  
  Master обрабатывает только операции записи, что снижает его нагрузку и повышает стабильность работы.

- **Масштабируемость без остановки сервиса**  
  При росте нагрузки можно добавлять новые slave-серверы без изменения логики работы приложения и без простоя системы.

---

## Вывод

Использование схем масштабирования с master и slave-серверами позволяет значительно повысить производительность, отказоустойчивость и гибкость системы.  
Один slave повышает надёжность и упрощает обслуживание, а несколько slave-серверов обеспечивают эффективное горизонтальное масштабирование и устойчивость к росту нагрузки.



### Задание 2

Разработайте план для выполнения горизонтального и вертикального шаринга базы данных. База данных состоит из трёх таблиц:

пользователи,
книги,
магазины (столбцы произвольно).
Опишите принципы построения системы и их разграничение или разбивку между базами данных.

`Ответ:`

План горизонтального и вертикального шардинга БД (пользователи, книги, магазины)

Ниже представлен план построения системы с использованием **вертикального** и **горизонтального шардинга** для базы данных, состоящей из трёх сущностей: **users (пользователи)**, **books (книги)**, **shops (магазины)**.  
Цель: обеспечить масштабирование, отказоустойчивость и управляемость нагрузки.

---

## 1) Исходные сущности и примерные столбцы

### users
- `user_id` (PK, bigint, UUID/sequence)
- `email` (unique)
- `phone`
- `password_hash`
- `name`
- `created_at`
- `region_id` (опционально, для гео-логики)

### books
- `book_id` (PK)
- `isbn` (unique)
- `title`
- `author`
- `category`
- `price`
- `created_at`

### shops
- `shop_id` (PK)
- `name`
- `region_id`
- `address`
- `created_at`

> Примечание: в реальной системе почти всегда появляются таблицы связей (например, остатки книг в магазинах, заказы), но в рамках задания ограничиваемся тремя таблицами и их разбиением.

---

## 2) Вертикальный шардинг (Vertical Sharding)

### Принцип
Вертикальный шардинг делит систему по **функциональным доменам / сервисам**.  
Каждый домен получает **свою БД** и обслуживается независимо.

### Разделение по доменам
- **DB_USERS** — отвечает только за пользователей и авторизацию/профили
- **DB_BOOKS** — каталог книг, поиск, метаданные
- **DB_SHOPS** — магазины, адреса, регионы, справочники точек

### Преимущества
- Изоляция нагрузки (например, активный поиск по книгам не влияет на пользователей)
- Независимое масштабирование и обслуживание
- Упрощение прав доступа и безопасности
- Более ясные границы ответственности (domain-driven подход)

### Ограничения/компенсации
- JOIN между таблицами из разных БД невозможен “в лоб”
- Нужен слой агрегации: приложение, API-gateway, витрины (read-model), кеш

---

## 3) Горизонтальный шардинг (Horizontal Sharding)

### Принцип
Горизонтальный шардинг делит **строки одной таблицы** на несколько узлов по shard-key.

Ниже приведён практичный вариант, который обычно хорошо работает:

### 3.1 Шардирование users
- **Shard-key:** `user_id`
- **Алгоритм:** `user_id % N` (или Consistent Hashing при динамическом числе шардов)

Пример: 4 шарда
- USERS_S1: `user_id % 4 = 0`
- USERS_S2: `user_id % 4 = 1`
- USERS_S3: `user_id % 4 = 2`
- USERS_S4: `user_id % 4 = 3`

### 3.2 Шардирование books
Вариант A (частый): если каталог огромный и нагрузка в основном на чтение
- **Shard-key:** `book_id` или `isbn_hash`
- **Алгоритм:** `hash(isbn) % N`

Вариант B (гео/регион): если ассортимент сильно отличается по регионам
- **Shard-key:** `category` или `region_id` (но аккуратно, возможен перекос нагрузки)

### 3.3 Шардирование shops
Чаще всего магазины шардируют по региону, чтобы локальные запросы были “рядом”:
- **Shard-key:** `region_id`
- **Алгоритм:** диапазоны регионов или `region_id % N`

---

## 4) Разграничение и “где что лежит” (итоговая модель)

### Комбинированная схема
1) **Вертикально** разделяем на 3 БД: Users / Books / Shops  
2) **Горизонтально** внутри каждой БД делаем шарды, если таблица/нагрузка требует

Пример:
- Users: 4 шарда по `user_id`
- Books: 2 шарда по `hash(isbn)`
- Shops: 2 шарда по `region_id`

---

## 5) Режимы работы серверов (репликация и роли)

Для каждого шарда применяем репликацию **master–slave** (или primary–replica):

- **Primary (Master)**: принимает запись (INSERT/UPDATE/DELETE)
- **Replica (Slave)**: обслуживает чтение (SELECT), отчёты, бэкапы
- Переключение при отказе primary: promotion реплики (manual/automatic)

Рекомендуемый режим:
- **Запись → только в Primary**
- **Чтение → распределять по Replica** (read scaling)
- Для критичных чтений допускается “read-your-writes” через primary (если важно отсутствие лагов)

---

## 6) Маршрутизация запросов (Shard Router)

Нужен компонент, который по shard-key определяет, куда идти:
- **Shard Router / Proxy** (на уровне приложения или отдельный слой)
- Хранит карту шардов: `key-range` или `hash-ring`
- Поддерживает операции:
  - вычислить shard по ключу
  - выполнить запрос в нужной БД
  - (опционально) агрегировать ответы (fan-out) для глобальных выборок

---

## 7) Блок-схема размещения (что и где располагается)

> Ниже блок-схема в формате Mermaid (её можно вставить в Markdown-платформу, поддерживающую Mermaid).

```
mermaid
flowchart TB
  A[Client / Application] --> R[Shard Router / Data Access Layer]

  %% Vertical split
  R --> UDB[DB_USERS (vertical)]
  R --> BDB[DB_BOOKS (vertical)]
  R --> SDB[DB_SHOPS (vertical)]

  %% USERS shards
  subgraph DB_USERS_Shards[DB_USERS: Horizontal Shards by user_id]
    U1[USERS_S1 Primary] --> U1R[USERS_S1 Replica]
    U2[USERS_S2 Primary] --> U2R[USERS_S2 Replica]
    U3[USERS_S3 Primary] --> U3R[USERS_S3 Replica]
    U4[USERS_S4 Primary] --> U4R[USERS_S4 Replica]
  end
  UDB --> DB_USERS_Shards

  %% BOOKS shards
  subgraph DB_BOOKS_Shards[DB_BOOKS: Horizontal Shards by hash(isbn)/book_id]
    B1[BOOKS_S1 Primary] --> B1R[BOOKS_S1 Replica]
    B2[BOOKS_S2 Primary] --> B2R[BOOKS_S2 Replica]
  end
  BDB --> DB_BOOKS_Shards

  %% SHOPS shards
  subgraph DB_SHOPS_Shards[DB_SHOPS: Horizontal Shards by region_id]
    S1[SHOPS_S1 Primary] --> S1R[SHOPS_S1 Replica]
    S2[SHOPS_S2 Primary] --> S2R[SHOPS_S2 Replica]
  end
  SDB --> DB_SHOPS_Shards
```

Итоговое описание принципов

Вертикальный шардинг: разделяет данные по доменам (Users/Books/Shops) — проще масштабировать сервисы независимо.

Горизонтальный шардинг: делит строки внутри домена по shard-key (user_id, isbn_hash, region_id) — выдерживает рост объёма данных и нагрузки.

Репликация на каждом шарде: primary для записи, replica для чтения и резервирования.

Router/Proxy: обязательный элемент, который направляет запросы в нужный шард и режим (write/read).


































